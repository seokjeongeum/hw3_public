{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIqzcT6vyUJI"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQrUWcSeyUJm"
   },
   "source": [
    "<center><h3>**Welcome to the Language modeling Notebook.**</h3></center>\n",
    "\n",
    "In this assignment, you are going to train a neural network to **generate news headlines**.\n",
    "To reduce computational needs, we have reduced it to headlines about technology, and a handful of Tech giants.\n",
    "In this assignment you will:\n",
    "- Learn to preprocess raw text so it can be fed into an LSTM.\n",
    "- Make use of the LSTM library of Pytorch, to train a Language model to generate headlines\n",
    "- Use your network to generate headlines, and judge which headlines are likely or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCAzgVCryUJr"
   },
   "source": [
    "**What is a language model?**\n",
    "\n",
    "Language modeling is the task of assigning a probability to sentences in a language. Besides assigning a probability to each sequence of words, the language models also assigns a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words.\n",
    "â€” Page 105, __[Neural Network Methods in Natural Language Processing](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/)__, 2017.\n",
    "\n",
    "In terms of neural network, we are training a neural network to produce probabilities (classification) over a fixed vocabulary of words.\n",
    "Concretely, we are training a neural network to produce:\n",
    "$$ P ( w_{i+1} | w_1, w_2, w_3, ..., w_i), \\forall i \\in (1,n)$$\n",
    "\n",
    "** Why is language modeling important? **\n",
    "\n",
    "Language modeling is a core problem in NLP.\n",
    "\n",
    "Language models can either be used as a stand-alone to produce new text that matches the distribution of text the model is trained on, but can also be used at the front-end of a more sophisticated model to produce better results.\n",
    "\n",
    "Recently for example, the __[BERT](https://arxiv.org/abs/1810.04805)__ paper show-cased that pretraining a large neural network on a language modeling task can help improve state-of-the-art on many NLP tasks. \n",
    "\n",
    "How good can the generation of a Language model be?\n",
    "\n",
    "If you have not seen the post about GPT-2 by OpenAI, you should read some of the samples they generated from their language model __[here](https://blog.openai.com/better-language-models/#sample1)__.\n",
    "Because of computational restrictions, we will not achieve as good text production, but the same algorithm is at the core. They just use more data and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LL29FMhfyUJv"
   },
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OXaa4g1yUJx"
   },
   "source": [
    "Before starting, make sure you have all these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3248,
     "status": "ok",
     "timestamp": 1614531798003,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "g81IeWKxyYO-",
    "outputId": "6e559492-7654-4cb2-9306-0c3382b8ccd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segtok in ./.venv/lib/python3.10/site-packages (1.5.11)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.10/site-packages (from segtok) (2024.11.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install segtok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the first of the following two cells if you are running the homework locally, and run the second cell if you are running the homework in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVE=False\n",
    "root_folder = \"\"\n",
    "dataset_folder = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3213,
     "status": "ok",
     "timestamp": 1614531798008,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "se7xdZD7y-CL",
    "outputId": "b8e44982-3eee-4a94-aa23-be86f9ec6061"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# root_folder = \"/content/drive/My Drive/cs182_hw3/\"\n",
    "# dataset_folder = \"/content/drive/My Drive/cs182_hw3_public/dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N1twBPSLyUJz"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pPLGvVwByUJ3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in ./.venv/lib/python3.10/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch<2 in ./.venv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.10/site-packages (from torch<2) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch<2) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.10/site-packages (from torch<2) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.10/site-packages (from torch<2) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.10/site-packages (from torch<2) (11.7.99)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2) (75.8.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2) (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"numpy<2\"\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(root_folder)\n",
    "%pip install \"torch<2\"\n",
    "from segtok import tokenizer\n",
    "from collections import Counter\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from utils import validate_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQTfdCRdyUJ8"
   },
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dNGkjKmyUJ9"
   },
   "source": [
    "Make sure the dataset files are all in the `dataset` folder of the assignment.\n",
    "\n",
    " - If you are using this notebook locally: You should run the `download_data.sh` script.\n",
    " - If you are using the Colab version of the notebook, make sure that your Google Drive is mounted, and you verify from the file explorer in Colab that the files are viewable within `/content/drive/cs182_hw3_public/dataset/`\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4822,
     "status": "ok",
     "timestamp": 1614531799702,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "iLaRmt5uyUJ_",
    "outputId": "6d4b3711-3bb0-478f-b06e-cad96ad3d36d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 88568\n",
      "Number of validation samples: 946\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the data for the model\n",
    "# Run this before working on loading any of the additional data\n",
    "\n",
    "with open(dataset_folder+\"headline_generation_dataset_processed.json\", \"r\") as f:\n",
    "    d_released = json.load(f)\n",
    "\n",
    "with open(dataset_folder+\"headline_generation_vocabulary.txt\", \"r\",encoding='utf8') as f:\n",
    "    vocabulary = f.read().split(\"\\n\")\n",
    "w2i = {w: i for i, w in enumerate(vocabulary)} # Word to index\n",
    "i2w = {i: w for i, w in enumerate(vocabulary)} # Index to word\n",
    "unkI, padI, startI = w2i['UNK'], w2i['PAD'], w2i['<START>']\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "input_length = len(d_released[0]['numerized']) # The length of the first element in the dataset, they are all of the same length\n",
    "d_train = [d for d in d_released if d['cut'] == 'training']\n",
    "d_valid = [d for d in d_released if d['cut'] == 'validation']\n",
    "\n",
    "print(\"Number of training samples:\",len(d_train))\n",
    "print(\"Number of validation samples:\",len(d_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u31F-vXqyUKB"
   },
   "source": [
    "Now that we have loaded the data, let's inspect one of the elements. Each sample in our dataset is has a `numerized` vector, that contains the preprocessed headline. This vector is what we will feed in to the neural network. The field `numerized` corresponds to this list of tokens. The already loaded dictionary `vocabulary` maps token lists to the actual string. Use these elements to recover `title` key of entry 1001 in the training dataset.\n",
    "\n",
    "**TODO**: Write the numerized2text function in notebook_utils and inspect element 1001 in the training dataset (`entry = d_train[1001]`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GqsHaZNbyUKD"
   },
   "outputs": [],
   "source": [
    "def numerize_sequence(tokenized):\n",
    "    return [w2i.get(w, unkI) for w in tokenized]\n",
    "def pad_sequence(numerized, pad_index, to_length):\n",
    "    pad = numerized[:to_length]\n",
    "    padded = pad + [pad_index] * (to_length - len(pad))\n",
    "    mask = [w != pad_index for w in padded]\n",
    "    return padded, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4773,
     "status": "ok",
     "timestamp": 1614531799708,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "C23VMn5DyUKF",
    "outputId": "821756d4-de73-4bac-d52e-6205ecaa6ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversing the numerized: microsoft donates cloud computing ' worth $ 1 bn ' PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "From the `title` entry: Microsoft donates cloud computing 'worth $1 bn'\n"
     ]
    }
   ],
   "source": [
    "def numerized2text(numerized):\n",
    "    \"\"\"Converts an integer sequence in the vocabulary into a string corresponding to the title.\n",
    "\n",
    "    Arguments:\n",
    "        numerized: List[int]  -- The list of vocabulary indices corresponding to the string\n",
    "    Returns:\n",
    "        title: str -- The string corresponding to the numerized input, without padding.\n",
    "    \"\"\"\n",
    "    #####\n",
    "    # BEGIN YOUR CODE HERE\n",
    "    # Recover each word from the vocabulary in the list of indices in numerized, using the vocabulary variable\n",
    "    # Hint 1: Use the string.join() function to reconstruct a single string\n",
    "    # Hint 2: The objects and/or functions defined in above cells may be useful.\n",
    "    #####\n",
    "    converted_string = \" \".join(np.array(vocabulary)[numerized])\n",
    "    #####\n",
    "    # END YOUR CODE HERE\n",
    "    #####\n",
    "\n",
    "    return converted_string\n",
    "\n",
    "\n",
    "entry = d_train[1001]\n",
    "print(\"Reversing the numerized: \" + numerized2text(entry[\"numerized\"]))\n",
    "validate_to_array(numerized2text, (entry[\"numerized\"],), \"numerized2text\", root_folder)\n",
    "print(\"From the `title` entry: \" + entry[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrpgWj1FyUKH"
   },
   "source": [
    "In language modeling, we train a model to produce the next word in the sequence given all previously generated words. This has, in practice, two steps:\n",
    "\n",
    "\n",
    "    1. Adding a special <START> token to the start of the sequence for the input. This \"shifts\" the input to the right by one. We call this the \"source\" sequence\n",
    "    2. Making the network predict the original, unshifted version (we call this the \"target\" sequence)\n",
    "\n",
    "    \n",
    "Let's take an example. Say we want to train the network on the sentence: \"The cat is great.\"\n",
    "The input to the network will be \"`<START>` The cat is great.\" The target will be: \"The cat is great\".\n",
    "    \n",
    "Therefore the first prediction is to select the word \"The\" given the `<START>` token.\n",
    "The second prediction is to produce the word \"cat\" given the two tokens \"`<START>` The\".\n",
    "At each step, the network learns to predict the next word, given all previous ones.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO2ww8XPyUKK"
   },
   "source": [
    "Your next step is to write the build_batch function. Given a dataset, we select a random subset of samples, and will build the \"inputs\" and the \"targets\" of the batch, following the procedure we've described.\n",
    "\n",
    "**TODO**: write the build_batch function. We give you the structure, and you have to fill in where we have left things `your_code`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Kl0f7hKKyUKL"
   },
   "outputs": [],
   "source": [
    "def build_batch(dataset, indices):\n",
    "    \"\"\"Builds a batch of source and target elements from the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        dataset: List[db_element] -- A list of dataset elements\n",
    "        indices: List[int] -- A list of indices of the dataset to sample\n",
    "    Returns:\n",
    "        batch_input: List[List[int]] -- List of source sequences\n",
    "        batch_target: List[List[int]] -- List of target sequences\n",
    "        batch_target_mask: List[List[int]] -- List of target batch masks\n",
    "    \"\"\"\n",
    "    #####\n",
    "    # BEGIN YOUR CODE HERE\n",
    "    #####\n",
    "\n",
    "    # We get a list of indices we will choose from the dataset.\n",
    "    # indices = range(iteration*batch_size,(iteration+1)*batch_size)\n",
    "\n",
    "    # Recover what the entries for the batch are\n",
    "    batch = np.array(dataset)[indices]\n",
    "\n",
    "    # Get the raw numerized for this input, each element of the dataset has a 'numerized' key\n",
    "    batch_numerized = np.array([x[\"numerized\"] for x in batch])\n",
    "\n",
    "    # Create an array of startI that will be concatenated at position 1 for the input.\n",
    "    # Should be of shape (batch_size, 1)\n",
    "    start_tokens = np.full((len(indices), 1), startI)\n",
    "\n",
    "    # Concatenate the start_tokens with the rest of the input\n",
    "    # The np.concatenate function should be useful\n",
    "    # The output should now be [batch_size, sequence_length+1]\n",
    "    batch_input = np.concatenate([start_tokens, batch_numerized], axis=1)\n",
    "\n",
    "    # Remove the last word from each element in the batch\n",
    "    # To restore the [batch_size, sequence_length] size\n",
    "    batch_input = batch_input[:, :-1]\n",
    "\n",
    "    # The target should be the un-shifted numerized input\n",
    "    batch_target = batch_numerized\n",
    "\n",
    "    # The target-mask is a 0 or 1 filter to note which tokens are\n",
    "    # padding or not, to give the loss, so the model doesn't get rewarded for\n",
    "    # predicting PAD tokens.\n",
    "    batch_target_mask = np.array([a[\"mask\"] for a in batch])\n",
    "\n",
    "    #####\n",
    "    # END YOUR CODE HERE\n",
    "    #####\n",
    "\n",
    "    return batch_input, batch_target, batch_target_mask\n",
    "\n",
    "\n",
    "validate_to_array(build_batch, (d_train, range(100)), \"build_batch\", root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBbG3vC5yUKN"
   },
   "source": [
    "# Creating the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3Eg8rHGyUKP"
   },
   "source": [
    "Now that we've written the data pipelining, we are ready to write the Neural network.\n",
    "\n",
    "The steps to setting up a neural network to do Language modeling are:\n",
    "- Creating the placeholders for the model, where we can feed in our inputs and targets.\n",
    "- Creating an RNN of our choice, size, and with optional parameters\n",
    "- Using the RNN on our placeholder inputs.\n",
    "- Getting the output from the RNN, and projecting it into a vocabulary sized dimension, so that we can make word predictions.\n",
    "- Setting up the loss on the outputs so that the network learns to produce the correct words.\n",
    "- Finally, choosing an optimizer, and defining a training operation: using the optimizer to minimize the loss.\n",
    "\n",
    "We provide skeleton code for the model, you can fill in the `your_code` section. If you are unfamiliar with Pytorch, we provide some idea of what functions to look for, you should use the Pytorch online documentation.\n",
    "\n",
    "**TODO**: Fill in the LanguageModel in the language_model file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rKJ2D23GyUKR"
   },
   "outputs": [],
   "source": [
    "from language_model import LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz7ixZGKyUKT"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sDUBkpnyUKU"
   },
   "source": [
    "Your objective is to train the Language on the dataset you are provided to reach a **validation loss <= 5.50**\n",
    "\n",
    "**TODO**: Train your model so that it achieves a validation loss of <= 5.5. \n",
    "\n",
    "**Careful**: we will be testing this loss on an unreleased test set, so make sure to evaluate properly on a validation set and not overfit. You must save the model you want us to test under: models/final_language_model (the .index, .meta and .data files)\n",
    "\n",
    "**Advice**:\n",
    "- It should be possible to attain loss <= 5.50 with a 1-layer LSTM of size 256 or less.\n",
    "- You should not need more than 10 epochs to attain the threshold. More passes over the data can however give you a better model.\n",
    "- You can however try using:\n",
    "    - LSTM dropout (Pytorch has a layer for that)\n",
    "    - Multi-layer RNN cell (Pytorch has a layer for that)\n",
    "    - Change your optimizers, tune your learning_rate, use a learning rate schedule.\n",
    "    \n",
    "**Extra credit**:\n",
    "\n",
    "Get the loss below **validation loss <= 5.00** and get 5 points of extra-credit on this assignment. Get creative,\n",
    "\n",
    "but remember, what you do should work on our held-out test set to get the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4699,
     "status": "ok",
     "timestamp": 1614531799716,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "b01AZj0RyUKW",
    "outputId": "d3628056-7dbe-4bdb-eb3d-b6cb54393d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# We can create our model,\n",
    "# with parameters of our choosing.\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Setup the loss using cross-entropy loss.\n",
    "# The logits are the output_logits we've computed,\n",
    "# look at the pytorch docs for `CrossEntropyLoss` and `permute`\n",
    "# to align the axes correctly and to account for the masking properly.\n",
    "# The targets are the goal labels we are trying to match.\n",
    "# Note that if you directly take the mean of the loss tensor,\n",
    "# it will underestimate your loss! (why would that be?)\n",
    "# Lastly, there are a few valid forms of averaging token losses,\n",
    "# here we will take the mean of all non-mask tokens together.\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "def loss_fn(pred, target, mask):\n",
    "    pred = pred.permute(0, 2, 1)  # put the class probabilities in the middle\n",
    "    loss_tensor = criterion(pred, target)\n",
    "    loss_masked = loss_tensor * mask\n",
    "    loss_per_sample = loss_masked.sum() / mask.sum()\n",
    "    return loss_per_sample\n",
    "\n",
    "\n",
    "# The build_batch function outputs numpy, but our model is built in pytorch,\n",
    "# so you need to convert numpy to pytorch.\n",
    "# You also have to cast the masks into float32, target into long, and input into long.\n",
    "# Look at the `float` and `long` function.\n",
    "batch_to_torch = lambda b_in, b_target, b_mask: (\n",
    "    th.from_numpy(b_in).long(),\n",
    "    th.from_numpy(b_target).long(),\n",
    "    th.from_numpy(b_mask).float(),\n",
    ")\n",
    "\n",
    "\n",
    "# Look at the docs for torch.optim and pick an optimizer\n",
    "# And provide it with a start learning rate.\n",
    "optimizer_class = optim.AdamW\n",
    "lr = 1e-3\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "\n",
    "model_id = \"test1\"\n",
    "os.makedirs(root_folder + \"models/part1/\", exist_ok=True)\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "list_to_device = lambda th_obj: [tensor.to(device) for tensor in th_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fjIAKqbvg9ER"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(\n",
    "    vocab_size=vocab_size, rnn_size=hidden_size, num_layers=num_layers, dropout=dropout\n",
    ")\n",
    "optimizer = optimizer_class(model.parameters(), lr=lr, weight_decay=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 599508,
     "status": "ok",
     "timestamp": 1614532394555,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "sVDHLiOmyUKX",
    "outputId": "43a34a54-ae3b-4c9d-86e3-0e3c30506f02",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Iteration: 690 Loss: 6.1319678783416744 Accuracy: 0.12138126268982888: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Validation Loss: 5.8736724853515625\n",
      "Saved new best model with val loss 5.8737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Iteration: 690 Loss: 6.0703388214111325 Accuracy: 0.12148561477661132: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:21<00:00, 32.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Validation Loss: 5.835305690765381\n",
      "Saved new best model with val loss 5.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Iteration: 690 Loss: 6.033824491500854 Accuracy: 0.12844372913241386: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:21<00:00, 32.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Validation Loss: 5.794264316558838\n",
      "Saved new best model with val loss 5.7943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Iteration: 690 Loss: 5.994955539703369 Accuracy: 0.12547899633646012: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:22<00:00, 30.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Validation Loss: 5.760225296020508\n",
      "Saved new best model with val loss 5.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Iteration: 690 Loss: 5.9718162536621096 Accuracy: 0.1294025346636772: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:21<00:00, 32.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Validation Loss: 5.729812145233154\n",
      "Saved new best model with val loss 5.7298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Iteration: 690 Loss: 5.964282035827637 Accuracy: 0.1339194618165493: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:22<00:00, 31.43it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Validation Loss: 5.703686714172363\n",
      "Saved new best model with val loss 5.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Iteration: 690 Loss: 5.923934745788574 Accuracy: 0.13271935135126114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Validation Loss: 5.676356792449951\n",
      "Saved new best model with val loss 5.6764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Iteration: 690 Loss: 5.858956670761108 Accuracy: 0.1324831284582615: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 29.52it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Validation Loss: 5.652425765991211\n",
      "Saved new best model with val loss 5.6524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Iteration: 690 Loss: 5.878982305526733 Accuracy: 0.1368647575378418: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:26<00:00, 26.24it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Validation Loss: 5.632537841796875\n",
      "Saved new best model with val loss 5.6325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Iteration: 690 Loss: 5.766369581222534 Accuracy: 0.14264036044478418: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:28<00:00, 24.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Validation Loss: 5.60797643661499\n",
      "Saved new best model with val loss 5.6080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Iteration: 690 Loss: 5.826354312896728 Accuracy: 0.14417929351329803: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:27<00:00, 25.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Validation Loss: 5.58839225769043\n",
      "Saved new best model with val loss 5.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Iteration: 690 Loss: 5.775570011138916 Accuracy: 0.14372445344924928: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:25<00:00, 27.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Validation Loss: 5.568085193634033\n",
      "Saved new best model with val loss 5.5681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Iteration: 690 Loss: 5.763145399093628 Accuracy: 0.14018619507551194: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:26<00:00, 26.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Validation Loss: 5.551076889038086\n",
      "Saved new best model with val loss 5.5511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Iteration: 690 Loss: 5.7234416007995605 Accuracy: 0.14127732962369918: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:25<00:00, 27.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Validation Loss: 5.531661510467529\n",
      "Saved new best model with val loss 5.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Iteration: 690 Loss: 5.713720846176147 Accuracy: 0.14280691742897034: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 29.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Validation Loss: 5.513602256774902\n",
      "Saved new best model with val loss 5.5136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Iteration: 690 Loss: 5.705002641677856 Accuracy: 0.1458980545401573: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 29.43it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Validation Loss: 5.499938011169434\n",
      "Saved new best model with val loss 5.4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Iteration: 690 Loss: 5.667625427246094 Accuracy: 0.14761812537908553: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:24<00:00, 28.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Validation Loss: 5.486828804016113\n",
      "Saved new best model with val loss 5.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Iteration: 690 Loss: 5.713454627990723 Accuracy: 0.1415018692612648: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 28.90it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Validation Loss: 5.466463565826416\n",
      "Saved new best model with val loss 5.4665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Iteration: 690 Loss: 5.629257535934448 Accuracy: 0.15099240243434905: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:24<00:00, 28.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Validation Loss: 5.456913471221924\n",
      "Saved new best model with val loss 5.4569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Iteration: 690 Loss: 5.653025484085083 Accuracy: 0.1487760990858078: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 29.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Validation Loss: 5.440999984741211\n",
      "Saved new best model with val loss 5.4410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Iteration: 690 Loss: 5.618740892410278 Accuracy: 0.15524579286575318: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 29.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Validation Loss: 5.4317169189453125\n",
      "Saved new best model with val loss 5.4317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Iteration: 690 Loss: 5.647732877731324 Accuracy: 0.14403168261051177: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:24<00:00, 28.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Validation Loss: 5.42484188079834\n",
      "Saved new best model with val loss 5.4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Iteration: 690 Loss: 5.5970314025878904 Accuracy: 0.1524325877428055: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:24<00:00, 28.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Validation Loss: 5.410187244415283\n",
      "Saved new best model with val loss 5.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Iteration: 690 Loss: 5.588820600509644 Accuracy: 0.15065210908651352: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:22<00:00, 30.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Validation Loss: 5.398757457733154\n",
      "Saved new best model with val loss 5.3988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Iteration: 690 Loss: 5.639509296417236 Accuracy: 0.14225512892007827: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 29.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Validation Loss: 5.385733604431152\n",
      "Saved new best model with val loss 5.3857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Iteration: 690 Loss: 5.544202184677124 Accuracy: 0.15183613300323487: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:24<00:00, 28.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Validation Loss: 5.377673625946045\n",
      "Saved new best model with val loss 5.3777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Iteration: 690 Loss: 5.583007526397705 Accuracy: 0.15098831802606583: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:25<00:00, 27.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Validation Loss: 5.367212295532227\n",
      "Saved new best model with val loss 5.3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Iteration: 690 Loss: 5.530154991149902 Accuracy: 0.1534997895359993: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:24<00:00, 28.82it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Validation Loss: 5.358710289001465\n",
      "Saved new best model with val loss 5.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Iteration: 690 Loss: 5.562793159484864 Accuracy: 0.15360087156295776: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:29<00:00, 23.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Validation Loss: 5.34564208984375\n",
      "Saved new best model with val loss 5.3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Iteration: 690 Loss: 5.5287987232208256 Accuracy: 0.1548133447766304: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:24<00:00, 28.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Validation Loss: 5.336002826690674\n",
      "Saved new best model with val loss 5.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Iteration: 690 Loss: 5.498895120620728 Accuracy: 0.1580825001001358: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.95it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Validation Loss: 5.331347942352295\n",
      "Saved new best model with val loss 5.3313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Iteration: 690 Loss: 5.52697434425354 Accuracy: 0.15342593491077422: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.88it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Validation Loss: 5.31801700592041\n",
      "Saved new best model with val loss 5.3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Iteration: 690 Loss: 5.519826745986938 Accuracy: 0.15557309240102768: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Validation Loss: 5.310035228729248\n",
      "Saved new best model with val loss 5.3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Iteration: 690 Loss: 5.51539158821106 Accuracy: 0.15191710293292998: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.92it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Validation Loss: 5.302761554718018\n",
      "Saved new best model with val loss 5.3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Iteration: 690 Loss: 5.478180646896362 Accuracy: 0.1621485620737076: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:26<00:00, 26.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Validation Loss: 5.297129154205322\n",
      "Saved new best model with val loss 5.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Iteration: 690 Loss: 5.476265907287598 Accuracy: 0.1556912049651146: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:23<00:00, 29.95it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Validation Loss: 5.286157608032227\n",
      "Saved new best model with val loss 5.2862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Iteration: 690 Loss: 5.447385311126709 Accuracy: 0.15582208931446076: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Validation Loss: 5.280440330505371\n",
      "Saved new best model with val loss 5.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Iteration: 690 Loss: 5.43580117225647 Accuracy: 0.15921226143836975: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Validation Loss: 5.274022102355957\n",
      "Saved new best model with val loss 5.2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Iteration: 690 Loss: 5.401480770111084 Accuracy: 0.15850600749254226: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 32.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Validation Loss: 5.264351844787598\n",
      "Saved new best model with val loss 5.2644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Iteration: 690 Loss: 5.388906860351563 Accuracy: 0.1620214179158211: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.50it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Validation Loss: 5.257831573486328\n",
      "Saved new best model with val loss 5.2578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40 Iteration: 690 Loss: 5.415353918075562 Accuracy: 0.15919750034809113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 Validation Loss: 5.251143455505371\n",
      "Saved new best model with val loss 5.2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 41 Iteration: 690 Loss: 5.44761381149292 Accuracy: 0.15558430105447768: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.98it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 Validation Loss: 5.245546817779541\n",
      "Saved new best model with val loss 5.2455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 42 Iteration: 690 Loss: 5.36662540435791 Accuracy: 0.16321303844451904: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.44it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 Validation Loss: 5.2395219802856445\n",
      "Saved new best model with val loss 5.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 43 Iteration: 690 Loss: 5.392172718048096 Accuracy: 0.1592481553554535: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.81it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 Validation Loss: 5.235273838043213\n",
      "Saved new best model with val loss 5.2353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 44 Iteration: 690 Loss: 5.362756204605103 Accuracy: 0.16221764534711838: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 Validation Loss: 5.230124473571777\n",
      "Saved new best model with val loss 5.2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 45 Iteration: 690 Loss: 5.395526504516601 Accuracy: 0.1574579268693924: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 Validation Loss: 5.227705955505371\n",
      "Saved new best model with val loss 5.2277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 46 Iteration: 690 Loss: 5.39258713722229 Accuracy: 0.15831895619630815: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.92it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 Validation Loss: 5.216131687164307\n",
      "Saved new best model with val loss 5.2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 47 Iteration: 690 Loss: 5.359222793579102 Accuracy: 0.16452469378709794: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 Validation Loss: 5.210713863372803\n",
      "Saved new best model with val loss 5.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 48 Iteration: 690 Loss: 5.3504383087158205 Accuracy: 0.16837919801473616: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 Validation Loss: 5.2068986892700195\n",
      "Saved new best model with val loss 5.2069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 49 Iteration: 690 Loss: 5.381691789627075 Accuracy: 0.1671860173344612: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.09it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 Validation Loss: 5.198848724365234\n",
      "Saved new best model with val loss 5.1988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50 Iteration: 690 Loss: 5.379554700851441 Accuracy: 0.16103825569152833: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 Validation Loss: 5.1927971839904785\n",
      "Saved new best model with val loss 5.1928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 51 Iteration: 690 Loss: 5.307805156707763 Accuracy: 0.16701227575540542: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 Validation Loss: 5.189753532409668\n",
      "Saved new best model with val loss 5.1898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 52 Iteration: 690 Loss: 5.362269306182862 Accuracy: 0.15853222608566284: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 Validation Loss: 5.181978225708008\n",
      "Saved new best model with val loss 5.1820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 53 Iteration: 690 Loss: 5.334455060958862 Accuracy: 0.1679845407605171: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 Validation Loss: 5.178476333618164\n",
      "Saved new best model with val loss 5.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 54 Iteration: 690 Loss: 5.335708713531494 Accuracy: 0.15897155702114105: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 Validation Loss: 5.175323963165283\n",
      "Saved new best model with val loss 5.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 55 Iteration: 690 Loss: 5.315512180328369 Accuracy: 0.16441715359687806: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 Validation Loss: 5.17097282409668\n",
      "Saved new best model with val loss 5.1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 56 Iteration: 690 Loss: 5.329663801193237 Accuracy: 0.1663459375500679: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 Validation Loss: 5.169686794281006\n",
      "Saved new best model with val loss 5.1697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 57 Iteration: 690 Loss: 5.3691082954406735 Accuracy: 0.1621215224266052: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 Validation Loss: 5.161103248596191\n",
      "Saved new best model with val loss 5.1611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 58 Iteration: 690 Loss: 5.312686777114868 Accuracy: 0.16163143515586853: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 Validation Loss: 5.16357946395874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 59 Iteration: 690 Loss: 5.270036029815674 Accuracy: 0.1735748454928398: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.99it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 Validation Loss: 5.153796195983887\n",
      "Saved new best model with val loss 5.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 60 Iteration: 690 Loss: 5.304565525054931 Accuracy: 0.16843605786561966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 Validation Loss: 5.154327392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 61 Iteration: 690 Loss: 5.275889253616333 Accuracy: 0.16428953260183335: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 Validation Loss: 5.146648406982422\n",
      "Saved new best model with val loss 5.1466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 62 Iteration: 690 Loss: 5.2510308742523195 Accuracy: 0.16653278321027756: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 Validation Loss: 5.144391059875488\n",
      "Saved new best model with val loss 5.1444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 63 Iteration: 690 Loss: 5.317184638977051 Accuracy: 0.1613687828183174: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 Validation Loss: 5.13632345199585\n",
      "Saved new best model with val loss 5.1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 64 Iteration: 690 Loss: 5.238944959640503 Accuracy: 0.1706140384078026: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.23it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 Validation Loss: 5.137294292449951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 65 Iteration: 690 Loss: 5.229941892623901 Accuracy: 0.167493237555027: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.99it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 Validation Loss: 5.129665374755859\n",
      "Saved new best model with val loss 5.1297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 66 Iteration: 690 Loss: 5.250402450561523 Accuracy: 0.16840819120407105: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 Validation Loss: 5.12377405166626\n",
      "Saved new best model with val loss 5.1238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 67 Iteration: 690 Loss: 5.259288930892945 Accuracy: 0.16743122339248656: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 Validation Loss: 5.123744010925293\n",
      "Saved new best model with val loss 5.1237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 68 Iteration: 690 Loss: 5.258086729049682 Accuracy: 0.16135288625955582: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 Validation Loss: 5.114820957183838\n",
      "Saved new best model with val loss 5.1148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 69 Iteration: 690 Loss: 5.2620518684387205 Accuracy: 0.17078315764665603: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 Validation Loss: 5.114995956420898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 70 Iteration: 690 Loss: 5.194170665740967 Accuracy: 0.171940778195858: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.45it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 Validation Loss: 5.110679626464844\n",
      "Saved new best model with val loss 5.1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 71 Iteration: 690 Loss: 5.237292766571045 Accuracy: 0.1709013268351555: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.75it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 Validation Loss: 5.109731674194336\n",
      "Saved new best model with val loss 5.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 72 Iteration: 690 Loss: 5.240435457229614 Accuracy: 0.17283084094524384: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 Validation Loss: 5.106288909912109\n",
      "Saved new best model with val loss 5.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 73 Iteration: 690 Loss: 5.224604940414428 Accuracy: 0.17080100923776625: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 Validation Loss: 5.102226734161377\n",
      "Saved new best model with val loss 5.1022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 74 Iteration: 690 Loss: 5.193443679809571 Accuracy: 0.1721683621406555: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.33it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 Validation Loss: 5.101139068603516\n",
      "Saved new best model with val loss 5.1011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 75 Iteration: 690 Loss: 5.168806505203247 Accuracy: 0.16913099884986876: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 36.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 Validation Loss: 5.100174427032471\n",
      "Saved new best model with val loss 5.1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 76 Iteration: 690 Loss: 5.205361557006836 Accuracy: 0.1693915992975235: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 Validation Loss: 5.091855049133301\n",
      "Saved new best model with val loss 5.0919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 77 Iteration: 690 Loss: 5.190276479721069 Accuracy: 0.1733582556247711: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 36.76it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 Validation Loss: 5.087942600250244\n",
      "Saved new best model with val loss 5.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 78 Iteration: 690 Loss: 5.1544183731079105 Accuracy: 0.1757098063826561: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78 Validation Loss: 5.083929061889648\n",
      "Saved new best model with val loss 5.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 79 Iteration: 690 Loss: 5.197062730789185 Accuracy: 0.17282797992229462: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 Validation Loss: 5.081719398498535\n",
      "Saved new best model with val loss 5.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 80 Iteration: 690 Loss: 5.203112173080444 Accuracy: 0.17010480165481567: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 36.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 Validation Loss: 5.079762935638428\n",
      "Saved new best model with val loss 5.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 81 Iteration: 690 Loss: 5.189410400390625 Accuracy: 0.17284712046384812: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 Validation Loss: 5.073680877685547\n",
      "Saved new best model with val loss 5.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 82 Iteration: 690 Loss: 5.115394020080567 Accuracy: 0.17907176464796065: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 Validation Loss: 5.076968193054199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 83 Iteration: 690 Loss: 5.160304594039917 Accuracy: 0.17556862980127336: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 Validation Loss: 5.07266902923584\n",
      "Saved new best model with val loss 5.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 84 Iteration: 690 Loss: 5.192904996871948 Accuracy: 0.1704835742712021: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 38.16it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 Validation Loss: 5.075275421142578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 85 Iteration: 690 Loss: 5.147973728179932 Accuracy: 0.1755061998963356: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 36.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 Validation Loss: 5.066590309143066\n",
      "Saved new best model with val loss 5.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 86 Iteration: 690 Loss: 5.139589071273804 Accuracy: 0.17305618524551392: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 Validation Loss: 5.063330173492432\n",
      "Saved new best model with val loss 5.0633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 87 Iteration: 690 Loss: 5.100084447860718 Accuracy: 0.17856552600860595: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 36.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 Validation Loss: 5.061854362487793\n",
      "Saved new best model with val loss 5.0619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 88 Iteration: 690 Loss: 5.086931228637695 Accuracy: 0.1848041146993637: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 36.81it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 Validation Loss: 5.05889368057251\n",
      "Saved new best model with val loss 5.0589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 89 Iteration: 690 Loss: 5.19562873840332 Accuracy: 0.16556521207094194: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 36.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 Validation Loss: 5.0569634437561035\n",
      "Saved new best model with val loss 5.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 90 Iteration: 690 Loss: 5.168718099594116 Accuracy: 0.17122071981430054: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 Validation Loss: 5.055038928985596\n",
      "Saved new best model with val loss 5.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 91 Iteration: 690 Loss: 5.126359510421753 Accuracy: 0.18240851908922195: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 36.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 Validation Loss: 5.0518879890441895\n",
      "Saved new best model with val loss 5.0519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 92 Iteration: 690 Loss: 5.077395582199097 Accuracy: 0.18193917125463485: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 36.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 Validation Loss: 5.048563480377197\n",
      "Saved new best model with val loss 5.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 93 Iteration: 690 Loss: 5.14219069480896 Accuracy: 0.17767405956983567: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 Validation Loss: 5.049846172332764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 94 Iteration: 690 Loss: 5.120662593841553 Accuracy: 0.18115674704313278: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 Validation Loss: 5.045170783996582\n",
      "Saved new best model with val loss 5.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 95 Iteration: 690 Loss: 5.117073106765747 Accuracy: 0.17258478850126266: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 Validation Loss: 5.042312145233154\n",
      "Saved new best model with val loss 5.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 96 Iteration: 690 Loss: 5.083605003356934 Accuracy: 0.17361707538366317: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96 Validation Loss: 5.039815902709961\n",
      "Saved new best model with val loss 5.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 97 Iteration: 690 Loss: 5.104813909530639 Accuracy: 0.1747190147638321: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 36.73it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 Validation Loss: 5.038727283477783\n",
      "Saved new best model with val loss 5.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 98 Iteration: 690 Loss: 5.084809398651123 Accuracy: 0.17381160259246825: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 Validation Loss: 5.033129692077637\n",
      "Saved new best model with val loss 5.0331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Iteration: 690 Loss: 5.128803777694702 Accuracy: 0.17265601307153702: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 Validation Loss: 5.034183025360107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Iteration: 690 Loss: 5.097299289703369 Accuracy: 0.18209814876317978: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 36.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 Validation Loss: 5.029355049133301\n",
      "Saved new best model with val loss 5.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Iteration: 690 Loss: 5.0477681159973145 Accuracy: 0.18178484290838243: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 36.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101 Validation Loss: 5.031768798828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 102 Iteration: 690 Loss: 5.063678789138794 Accuracy: 0.1765623062849045: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.24it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102 Validation Loss: 5.028562068939209\n",
      "Saved new best model with val loss 5.0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 103 Iteration: 690 Loss: 5.135699319839477 Accuracy: 0.17221090495586394: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103 Validation Loss: 5.025163650512695\n",
      "Saved new best model with val loss 5.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 104 Iteration: 690 Loss: 5.1306990623474125 Accuracy: 0.1759599193930626: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104 Validation Loss: 5.024940013885498\n",
      "Saved new best model with val loss 5.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 105 Iteration: 690 Loss: 5.077016830444336 Accuracy: 0.17874182611703873: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105 Validation Loss: 5.019539833068848\n",
      "Saved new best model with val loss 5.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 106 Iteration: 690 Loss: 5.097570991516113 Accuracy: 0.17861033231019974: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106 Validation Loss: 5.022732734680176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 107 Iteration: 690 Loss: 5.090868616104126 Accuracy: 0.17380513101816178: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 37.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107 Validation Loss: 5.018594264984131\n",
      "Saved new best model with val loss 5.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 108 Iteration: 690 Loss: 5.051522302627563 Accuracy: 0.1832921177148819: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:18<00:00, 36.59it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108 Validation Loss: 5.020631313323975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 109 Iteration: 690 Loss: 5.075745105743408 Accuracy: 0.17875033468008042: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 35.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109 Validation Loss: 5.016787528991699\n",
      "Saved new best model with val loss 5.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 110 Iteration: 690 Loss: 5.0380988121032715 Accuracy: 0.17348500341176987: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110 Validation Loss: 5.018561363220215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 111 Iteration: 690 Loss: 5.041338729858398 Accuracy: 0.1729518309235573: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.62it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111 Validation Loss: 5.01122522354126\n",
      "Saved new best model with val loss 5.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 112 Iteration: 690 Loss: 5.063911724090576 Accuracy: 0.17614165246486663: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:19<00:00, 34.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112 Validation Loss: 5.012146472930908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 113 Iteration: 690 Loss: 5.072928810119629 Accuracy: 0.1794956922531128: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.47it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113 Validation Loss: 5.006381034851074\n",
      "Saved new best model with val loss 5.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 114 Iteration: 690 Loss: 5.042406225204468 Accuracy: 0.17670904844999313: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114 Validation Loss: 5.007034778594971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 115 Iteration: 690 Loss: 5.048744201660156 Accuracy: 0.18068117499351502: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115 Validation Loss: 5.004249572753906\n",
      "Saved new best model with val loss 5.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 116 Iteration: 690 Loss: 5.035372018814087 Accuracy: 0.1800599843263626: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:26<00:00, 26.33it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116 Validation Loss: 5.001808166503906\n",
      "Saved new best model with val loss 5.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 117 Iteration: 690 Loss: 5.0279004096984865 Accuracy: 0.18048894554376602: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 34.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117 Validation Loss: 5.001772880554199\n",
      "Saved new best model with val loss 5.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 118 Iteration: 690 Loss: 5.081632804870606 Accuracy: 0.17443606108427048: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 692/692 [00:20<00:00, 33.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118 Validation Loss: 4.996859550476074\n",
      "Saved new best model with val loss 4.9969\n"
     ]
    }
   ],
   "source": [
    "# Skeleton code\n",
    "# You have to write your own training process to obtain a\n",
    "# Good performing model on the validation set, and save it.\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "accuracies = []\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "p = Path(root_folder + \"models/part1/\" + f\"model_{model_id}.pt\")\n",
    "best_valid_loss = float(\"inf\")\n",
    "if p.exists():\n",
    "    save_dict = th.load(\n",
    "        root_folder + \"models/part1/\" + f\"model_{model_id}.pt\", map_location=\"cpu\"\n",
    "    )\n",
    "    model.load_state_dict(save_dict[\"model_state_dict\"])\n",
    "    batch = build_batch(d_valid, range(len(d_valid)))\n",
    "    (batch_input, batch_target, batch_target_mask) = batch_to_torch(*batch)\n",
    "    (batch_input, batch_target, batch_target_mask) = list_to_device(\n",
    "        (batch_input, batch_target, batch_target_mask)\n",
    "    )\n",
    "    prediction = model(batch_input)\n",
    "    best_valid_loss = loss_fn(prediction, batch_target, batch_target_mask)\n",
    "# Add cosine annealing\n",
    "scheduler = th.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    if best_valid_loss <= 5.00:\n",
    "        break\n",
    "    model.train()\n",
    "    indices = np.random.permutation(range(len(d_train)))\n",
    "    t = tqdm(range(0, (len(d_train) // batch_size) + 1))\n",
    "    for i in t:\n",
    "        # Here is how you obtain a batch:\n",
    "        batch = build_batch(d_train, indices[i * batch_size : (i + 1) * batch_size])\n",
    "        (batch_input, batch_target, batch_target_mask) = batch_to_torch(*batch)\n",
    "        (batch_input, batch_target, batch_target_mask) = list_to_device(\n",
    "            (batch_input, batch_target, batch_target_mask)\n",
    "        )\n",
    "\n",
    "        prediction = model(batch_input)\n",
    "        loss = loss_fn(prediction, batch_target, batch_target_mask)\n",
    "        losses.append(loss.item())\n",
    "        accuracy = (\n",
    "            th.eq(prediction.argmax(dim=2, keepdim=False), batch_target).float()\n",
    "            * batch_target_mask\n",
    "        ).sum() / batch_target_mask.sum()\n",
    "        accuracies.append(accuracy.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 10 == 0:\n",
    "            t.set_description(\n",
    "                f\"Epoch: {epoch} Iteration: {i} Loss: {np.mean(losses[-10:])} Accuracy: {np.mean(accuracies[-10:])}\"\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    batch = build_batch(d_valid, range(len(d_valid)))\n",
    "    (batch_input, batch_target, batch_target_mask) = batch_to_torch(*batch)\n",
    "    (batch_input, batch_target, batch_target_mask) = list_to_device(\n",
    "        (batch_input, batch_target, batch_target_mask)\n",
    "    )\n",
    "    prediction = model(batch_input)\n",
    "    valid_loss = loss_fn(prediction, batch_target, batch_target_mask)\n",
    "    print(\"Epoch:\", epoch, \"Validation Loss:\", valid_loss.item())\n",
    "    # Save the model if the validation loss decreases\n",
    "    if best_valid_loss > valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # save your latest model\n",
    "        save_dict = dict(\n",
    "            kwargs=dict(\n",
    "                vocab_size=vocab_size,\n",
    "                rnn_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                dropout=dropout,\n",
    "            ),\n",
    "            model_state_dict=model.state_dict(),\n",
    "            notes=\"\",\n",
    "            optimizer_class=optimizer_class,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        th.save(save_dict, root_folder + f\"models/part1/model_{model_id}.pt\")\n",
    "        print(f\"Saved new best model with val loss {valid_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhaj6XIdyUKZ"
   },
   "source": [
    "# Using the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lgsKMqmyUKa"
   },
   "source": [
    "Congratulations, you have now trained a language model! We can now use it to evaluate likely news headlines, as well as generate our very own headlines.\n",
    "\n",
    "**TODO**: Complete the three parts below, using the model you have trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnR8ma3cyUKb"
   },
   "source": [
    "## (1) Evaluation loss\n",
    "\n",
    "To evaluate the language model, we evaluate its loss (ability to predict) on unseen data that is reserved for evaluation.\n",
    "Your first evaluation is to load the model you trained, and obtain a test loss. If you are running this validation and not training, run the setup cell above the training loop first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1614533226460,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "zkL164RCyUKc",
    "outputId": "65278858-abba-4c96-ccc0-26900f798626"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (embedding): Embedding(10000, 256)\n",
       "  (embed_dropout): Dropout(p=0.35, inplace=False)\n",
       "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (output): Linear(in_features=256, out_features=10000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"test1\"\n",
    "save_dict = th.load(root_folder+'models/part1/'+f\"model_{model_id}.pt\",map_location='cpu')\n",
    "model = LanguageModel(**save_dict['kwargs'])\n",
    "model.load_state_dict(save_dict['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10236,
     "status": "ok",
     "timestamp": 1614533236326,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "o0B71owvyUKe",
    "outputId": "cabc3e8d-833d-48f6-9366-596300db020c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation set loss: 4.996857643127441\n"
     ]
    }
   ],
   "source": [
    "# We will evaluate your model in the best_models folder\n",
    "# In a very similar way as the code below.\n",
    "# Make sure your validation loss is below the threshold we specified\n",
    "# and that you didn't train using the validation set\n",
    "\n",
    "batch = build_batch(d_valid, range(len(d_valid)))\n",
    "(batch_input, batch_target, batch_target_mask) = batch_to_torch(*batch)\n",
    "prediction = model(batch_input.long())\n",
    "loss = loss_fn(prediction, batch_target, batch_target_mask)\n",
    "print(\"Evaluation set loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "2W-FFdHoyUKg"
   },
   "outputs": [],
   "source": [
    "# Your best performing model should go here.\n",
    "os.makedirs(root_folder+\"best_models\",exist_ok=True)\n",
    "best_model_file = root_folder+\"best_models/part1_best_model.pt\"\n",
    "th.save(save_dict,best_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oP9wBwfyUKj"
   },
   "source": [
    "## (2) Evaluation of likelihood of data\n",
    "\n",
    "One use of a language model is to see what data is more likely to have originated from the training data. Because we have trained our model on news headlines, we can see which of these headlines is more likely:\n",
    "\n",
    "``Apple to release another iPhone in September``\n",
    "\n",
    "\n",
    " ``Apple and Samsung resolve all lawsuits amicably``\n",
    " \n",
    "**TODO**: Use the model to obtain the loss the neural network assigns to each sentence.\n",
    "Because the neural network assigns probability to the words appearing in a sequence, this loss can be used as a proxy to measure how likely the sentence is to have occurred in the dataset.\n",
    "Once you have the loss for each headline, write down which sentence was judged to be more likely, and explain why/if you think this is coherent.\n",
    "\n",
    "**Your answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "CH-YlqE2yUKk"
   },
   "outputs": [],
   "source": [
    "def raw_sample_pred(headline, model):\n",
    "    #####\n",
    "    # BEGIN YOUR CODE HERE \n",
    "    #####\n",
    "    # From the code in the Preprocessing section at the end of the notebook\n",
    "    # Find out how to tokenize the headline\n",
    "    tokenized = your_code\n",
    "\n",
    "    # Find out how to numerize the tokenized headline\n",
    "    numerized = your_code\n",
    "\n",
    "    # Learn how to pad and obtain the mask of the sequence.\n",
    "    padded, mask = your_code\n",
    "\n",
    "    # Obtain the predicted headline and target headline\n",
    "    input_headline = your_code\n",
    "    pred_headline = your_code\n",
    "    target_headline = your_code\n",
    "    mask = your_code\n",
    "\n",
    "    #####\n",
    "    # END YOUR CODE HERE \n",
    "    #####\n",
    "\n",
    "    return pred_headline,target_headline,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1614533244496,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "LexHAsuiyUKm",
    "outputId": "fdfbbbe8-7da4-4f0a-8323-52194d30988d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'your_code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m headlines \u001b[38;5;241m=\u001b[39m [headline1\u001b[38;5;241m.\u001b[39mlower(), headline2\u001b[38;5;241m.\u001b[39mlower()] \u001b[38;5;66;03m# Our LSTM is trained on lower-cased headlines\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m headline \u001b[38;5;129;01min\u001b[39;00m headlines:\n\u001b[0;32m----> 8\u001b[0m     pred_headline,target_headline,mask \u001b[38;5;241m=\u001b[39m \u001b[43mraw_sample_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m your_code \u001b[38;5;66;03m# Obtain the loss\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 7\u001b[0m, in \u001b[0;36mraw_sample_pred\u001b[0;34m(headline, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraw_sample_pred\u001b[39m(headline, model):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# BEGIN YOUR CODE HERE \u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# From the code in the Preprocessing section at the end of the notebook\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Find out how to tokenize the headline\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m \u001b[43myour_code\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Find out how to numerize the tokenized headline\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     numerized \u001b[38;5;241m=\u001b[39m your_code\n",
      "\u001b[0;31mNameError\u001b[0m: name 'your_code' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "headline1 = \"Apple to release new iPhone in July\"\n",
    "headline2 = \"Apple and Samsung resolve all lawsuits\"\n",
    "\n",
    "headlines = [headline1.lower(), headline2.lower()] # Our LSTM is trained on lower-cased headlines\n",
    "for headline in headlines:\n",
    "    pred_headline,target_headline,mask = raw_sample_pred(headline, model)\n",
    "    loss = your_code # Obtain the loss\n",
    "    \n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Headline:\", headline)\n",
    "    print(\"Loss of the headline:\", loss)\n",
    "validate_to_array(raw_sample_pred,zip(headlines,[model]*2),'raw_sample_pred',root_folder,multi=True)\n",
    "# Important check: one headline should be more likely (and have lower loss)\n",
    "# Than the other headline. You should know which headline should have lower loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpn8XlLXyUKn"
   },
   "source": [
    "## (3) Generation of headlines\n",
    "\n",
    "We can use our language model to generate text according to the distribution of our training data.\n",
    "The way generation works is the following:\n",
    "\n",
    "We seed the model with a beginning of sequence, and obtain the distribution for the next word.\n",
    "We select the most likely word (argmax) and add it to our sequence of words.\n",
    "Now our sequence is one word longer, and we can feed it in again as an input, for the network to produce the next sentence.\n",
    "We do this a fixed number of times (up to 20 words), and obtain automatically generated headlines!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hycdvFs6yUKo"
   },
   "source": [
    "We have provided a few headline starters that should produce interesting generated headlines.\n",
    "\n",
    "**TODO:** Get creative and find at least 2 more headline_starters that produce interesting headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCYvwLw1yUKp"
   },
   "outputs": [],
   "source": [
    "def generate_sentence(headline_starter, model):\n",
    "    # Tokenize and numerize the headline. Put the numerized headline\n",
    "    # beginning in `current_build`\n",
    "    tokenized = tokenizer.word_tokenizer(headline_starter.lower())\n",
    "    current_build = [startI] + numerize_sequence(tokenized)\n",
    "\n",
    "    while len(current_build) < input_length:\n",
    "        # Pad the current_build into a input_length vector.\n",
    "        # We do this so that it can be processed by our LanguageModel class\n",
    "        current_padded, _m = pad_sequence(current_build, padI, input_length)\n",
    "\n",
    "        # Obtain the logits for the current padded sequence\n",
    "        # This involves obtaining the output_logits from our model,\n",
    "        # and not the loss like we have done so far\n",
    "        logits = your_code\n",
    "        logits_np = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Obtain the row of logits that interest us, the logits for the last non-pad\n",
    "        # inputs\n",
    "        last_logits = your_code\n",
    "\n",
    "        # Find the highest scoring words in the last_logits\n",
    "        # array, or sample from the softmax.\n",
    "        # The np.argmax function may be useful for first option,\n",
    "        # sp.special.softmax and np.random.choice may be useful for second option.\n",
    "        # Append this word to our current build\n",
    "        current_build.append(your_code)\n",
    "\n",
    "    # Go from the current_build of word_indices\n",
    "    # To the headline (string) produced. This should involve\n",
    "    # the vocabulary, and a string merger.\n",
    "    produced_sentence = your_code\n",
    "    return produced_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3069,
     "status": "ok",
     "timestamp": 1614533251841,
     "user": {
      "displayName": "Sean Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjK8rkabs7CsaHjMrOJ7lF9JD8ld_9_f9SxLG76=s64",
      "userId": "07641202443553730242"
     },
     "user_tz": 480
    },
    "id": "xVKeMmSAyUKq",
    "outputId": "75862bb0-787d-4aa4-9df5-4dfbc0c35e2d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Here are some headline starters.\n",
    "# They're all about tech companies, because\n",
    "# That is what is in our dataset\n",
    "headline_starters = [\"apple has released\", \"google has released\", \"amazon\", \"tesla to\"]\n",
    "for headline_starter in headline_starters:\n",
    "    print(\"===================\")\n",
    "    print(\"Generating headline starting with: \"+headline_starter)\n",
    "\n",
    "    produced_sentence = generate_sentence(headline_starter, model)\n",
    "    print(produced_sentence)\n",
    "validate_to_array(generate_sentence,zip(headline_starters,[model]*len(headline_starters)),\"generate_sentence\",root_folder,multi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ6ni5GZyUKr"
   },
   "source": [
    "## All done\n",
    "\n",
    "You are done with the first part of the HW.\n",
    "\n",
    "Next notebook deals with Summarization of text!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "QoPgKURxyUKs"
   },
   "source": [
    "# Preprocessing (read only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "FfOq00MZyUKt"
   },
   "source": [
    "**You can skip this section, however you may find these functions useful in later sections of this notebook**\n",
    "\n",
    "We have provided this code so you see how the dataset was generated. You will have to come back some of these functions later in the assignment, so feel free to read through, to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "uS8FUmo9yUKt"
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "for a in dataset:\n",
    "    a['tokenized'] = tokenizer.word_tokenizer(a['title'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "g_9okOHoyUKu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "word_counts = Counter()\n",
    "for a in dataset:\n",
    "    word_counts.update(a['tokenized'])\n",
    "\n",
    "print(word_counts.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "OhNeO1LdyUKv"
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "# Creating the vocab\n",
    "vocab_size = 20000\n",
    "special_words = [\"<START>\", \"UNK\", \"PAD\"]\n",
    "vocabulary = special_words + [w for w, c in word_counts.most_common(vocab_size-len(special_words))]\n",
    "w2i = {w: i for i, w in enumerate(vocabulary)}\n",
    "\n",
    "# Numerizing and padding\n",
    "input_length = 20\n",
    "unkI, padI, startI = w2i['UNK'], w2i['PAD'], w2i['<START>']\n",
    "\n",
    "for a in dataset:\n",
    "    a['numerized'] = numerize_sequence(a['tokenized']) # Change words to IDs\n",
    "    a['numerized'], a['mask'] = pad_sequence(a['numerized'], padI, input_length) # Append appropriate PAD tokens\n",
    "    \n",
    "# Compute fraction of words that are UNK:\n",
    "word_counters = Counter([w for a in dataset for w in a['input'] if w != padI])\n",
    "\n",
    "print(\"Fraction of UNK words:\", float(word_counters[unkI]) / sum(word_counters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "rpeDdDrUyUKw"
   },
   "outputs": [],
   "source": [
    "# You do not need to run this\n",
    "# This is to show you how the dataset was created\n",
    "# You should read to understand, so you can preprocess text\n",
    "# In the same way, in the evaluation section\n",
    "\n",
    "d_released_processed   = [d for d in dataset if d['cut'] != 'testing']\n",
    "d_unreleased_processed = [d for d in dataset if d['cut'] == 'testing']\n",
    "\n",
    "with open(\"dataset/headline_generation_dataset_processed.json\", \"w\") as f:\n",
    "    json.dump(d_released_processed, f)\n",
    "\n",
    "# This file is purposefully left out of the assignment, we will use it to evaluate your model.\n",
    "with open(\"dataset/headline_generation_dataset_unreleased_processed.json\", \"w\") as f:\n",
    "    json.dump(d_unreleased_processed, f)\n",
    "    \n",
    "with open(\"dataset/headline_generation_vocabulary.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(vocabulary).encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS--8I0KlDvO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "1 Language Modeling.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
